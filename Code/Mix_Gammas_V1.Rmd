---
title: "Mix_Gammas_V1"
author: "Xi Chen"
date: "June 11, 2016"
output: pdf_document
---
## Read in the gammamixEM2.R file
```{r setup, echo=FALSE}
source("./Code/gammamixEM2.R")
set.seed(518)
library("mixtools")
library("MASS")
```

## Consider 12 settings and for each of the 12 conditions, do the follwoing:
Number of samples: n.iter (B)  = 5000 samples of size n (Sample size: n). 

```{r}
# sample.size = 5
n.iter = 5000 # this is B
sample.size = c(100, 250, 500)

parameters <- function(){
        conditions <- list()
        # Condition 1
        conditions[["C1"]] <- data.frame(c(2,5),c(3,4),c(0.5,0.5))
        colnames(conditions[["C1"]]) <- c("a", "b", "l")
        # Condition 2
        conditions[["C2"]] <- data.frame(c(2,5),c(3,4),c(0.2,0.8))
        colnames(conditions[["C2"]]) <- c("a", "b", "l")
        # Condition 3
        conditions[["C3"]] <- data.frame(c(1,10),c(1,1),c(0.5,0.5))
        colnames(conditions[["C3"]]) <- c("a", "b", "l")
        # Condition 4
        conditions[["C4"]] <- data.frame(c(1,10),c(1,1),c(0.2,0.8))
        colnames(conditions[["C4"]]) <- c("a", "b", "l")
        # Condition 5
        conditions[["C5"]] <- data.frame(c(2,30),c(3,2),c(0.5,0.5))
        colnames(conditions[["C5"]]) <- c("a", "b", "l")
        # Condition 6
        conditions[["C6"]] <- data.frame(c(2,30),c(3,2),c(0.2,0.8))
        colnames(conditions[["C6"]]) <- c("a", "b", "l")
        # Condition 7
        conditions[["C7"]] <- data.frame(c(2,5,6),c(3,5,7),c(1/3,1/3,1/3))
        colnames(conditions[["C7"]]) <- c("a", "b", "l")
        # Condition 8
        conditions[["C8"]] <- data.frame(c(2,5,6),c(3,5,7),c(0.2,0.3,0.5))
        colnames(conditions[["C8"]]) <- c("a", "b", "l")
        # Condition 9
        conditions[["C9"]] <- data.frame(c(1,20,50),c(2,4,3),c(0.2,0.3,0.5))
        colnames(conditions[["C9"]]) <- c("a", "b", "l")
        # Condition 10
        conditions[["C10"]] <- data.frame(c(1,20,50),c(2,4,3),c(0.2,0.3,0.5))
        colnames(conditions[["C10"]]) <- c("a", "b", "l")
        # Condition 11
        conditions[["C11"]] <- data.frame(c(2,50,180),c(1,2,3),c(0.2,0.3,0.5))
        colnames(conditions[["C11"]]) <- c("a", "b", "l")
        # Condition 12
        conditions[["C12"]] <- data.frame(c(2,50,180),c(1,2,3),c(0.2,0.3,0.5))
        colnames(conditions[["C12"]]) <- c("a", "b", "l")
        return(conditions)
}
conditions <- parameters()

```

## Generates Sample:
We are using the rate not beta, which rate = 1/beta

```{r}
set.seed(111)
sample.generation <- function(sample.size, parameters){
        samples <- apply(parameters, 1, function(i){rgamma(sample.size* i[3], shape = i[1], scale = i[2])
        })
        return(unlist(samples))
}

sample <- sample.generation(100, conditions[["C12"]])


```

## Estimation:
For each set of samples, estimate the mixture-of-gammas model using 4 different stragegies. For each of the strategy, keep 

### Strategies 1
Specify the starting values in gammamixEM2 using the parameter values for the simulation.

```{r}
estimation1.f <- function(dat, para){
        a <- para$a
        b <- para$b
        l <- para$l
        numOfDist <- nrow(para)
        output <- gammamixEM2(dat, lambda = l, alpha = a, beta = b, k = numOfDist,)
        return(output[2:4])
}

test1 <- estimation1.f(sample, conditions[["C12"]])
test1
```

### Strategies 2
Do not specify starting values for any of the parameters in gammamixEM2. Run the algorithm 10 times and retain the output that has the best log-likelihood value; i.e. the fit that has the largest log-likelihood value.
```{r}
Rprof("profile2.out")
estimation2.f <- function(dat, para){
        numOfDist <- nrow(para)
        output <- gammamixEM2(dat, k = numOfDist,)
        return(output[2:4])
}

test2 <- estimation2.f(sample, conditions[["C12"]])
test2
Rprof(NULL)

library(profr)
ggplot.profr(parse_rprof("profile2.out"))
```
### Strategies 3
* Transform simulated data by taking the cub root, using normalmixEM to classify each observation to a component, which will effectively partition the simulated data into k groups. 

* Do this classfication
* Meeting June 22, 16: 
   * The fitdist() function from tent's code might not stable, so we decided to use the gamma.nr(), used it to estimate the parameter after classification and transform back to gamma with ^3. 
   
   
   
################ Need to ask which is parameter needs to pass in, and eps is tolerance what is the value?
```{r}
source("./Code/nr_gamma.R")
Rprof("profile3.out")
dat = sample
numOfDist = 3
estimation3.f <- function(dat, para){
        datCubeRoot <- dat^(1/3)
        numOfDist <- nrow(para)
        classification <- normalmixEM(datCubeRoot, k = numOfDist) ## The classified data are clustered and not like real classified.
        clf.dat <- apply((classification$posterior == T), 2, function(x) dat[x] )
        a.est <- classification$mu
        b.est <- classification$sigma
        l.est <- classification$lambda
        # nr.gamma(x, alpha = a, eps = b, lambda = l)
        para.est <- lapply(c(1:numOfDist), function(i){
                print(i)
                nr.gamma(x=unlist(clf.dat[i]), alpha = a.est[i], eps = 0.01, lambda = l.est[i])
                }) 
        
        output <- gammamixEM2(dat, lambda = l, alpha = a, beta = b, k = numOfDist,)
        return(para.est)
}
test3 <- estimation3.f(sample, conditions[["C12"]])
test3
Rprof(NULL)

library(profr)
ggplot.profr(parse_rprof("profile3.out"))
```
### Strategies 4
Re do the strategy 3, but set $alpha$ to the true parameter values and set $fix.alpha=TRUE$ in $gammamixEM2$.
```{r}

```

