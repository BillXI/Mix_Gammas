{
    "contents" : "---\ntitle: \"Mix_Gammas_V1\"\nauthor: \"Xi Chen\"\ndate: \"June 11, 2016\"\noutput: pdf_document\n---\n## Read in the gammamixEM2.R file\n```{r setup, echo=FALSE}\nsource(\"./Code/gammamixEM2.R\")\nset.seed(518)\nlibrary(\"mixtools\")\nlibrary(\"MASS\")\n```\n\n## Consider 12 settings and for each of the 12 conditions, do the follwoing:\nNumber of samples: n.iter (B)  = 5000 samples of size n (Sample size: n). \n\n```{r}\n# sample.size = 5\nn.iter = 5000 # this is B\nsample.size = c(100, 250, 500)\n\nparameters <- function(){\n        conditions <- list()\n        # Condition 1\n        conditions[[\"C1\"]] <- data.frame(c(2,5),c(3,4),c(0.5,0.5))\n        colnames(conditions[[\"C1\"]]) <- c(\"a\", \"b\", \"l\")\n        # Condition 2\n        conditions[[\"C2\"]] <- data.frame(c(2,5),c(3,4),c(0.2,0.8))\n        colnames(conditions[[\"C2\"]]) <- c(\"a\", \"b\", \"l\")\n        # Condition 3\n        conditions[[\"C3\"]] <- data.frame(c(1,10),c(1,1),c(0.5,0.5))\n        colnames(conditions[[\"C3\"]]) <- c(\"a\", \"b\", \"l\")\n        # Condition 4\n        conditions[[\"C4\"]] <- data.frame(c(1,10),c(1,1),c(0.2,0.8))\n        colnames(conditions[[\"C4\"]]) <- c(\"a\", \"b\", \"l\")\n        # Condition 5\n        conditions[[\"C5\"]] <- data.frame(c(2,30),c(3,2),c(0.5,0.5))\n        colnames(conditions[[\"C5\"]]) <- c(\"a\", \"b\", \"l\")\n        # Condition 6\n        conditions[[\"C6\"]] <- data.frame(c(2,30),c(3,2),c(0.2,0.8))\n        colnames(conditions[[\"C6\"]]) <- c(\"a\", \"b\", \"l\")\n        # Condition 7\n        conditions[[\"C7\"]] <- data.frame(c(2,5,6),c(3,5,7),c(1/3,1/3,1/3))\n        colnames(conditions[[\"C7\"]]) <- c(\"a\", \"b\", \"l\")\n        # Condition 8\n        conditions[[\"C8\"]] <- data.frame(c(2,5,6),c(3,5,7),c(0.2,0.3,0.5))\n        colnames(conditions[[\"C8\"]]) <- c(\"a\", \"b\", \"l\")\n        # Condition 9\n        conditions[[\"C9\"]] <- data.frame(c(1,20,50),c(2,4,3),c(0.2,0.3,0.5))\n        colnames(conditions[[\"C9\"]]) <- c(\"a\", \"b\", \"l\")\n        # Condition 10\n        conditions[[\"C10\"]] <- data.frame(c(1,20,50),c(2,4,3),c(0.2,0.3,0.5))\n        colnames(conditions[[\"C10\"]]) <- c(\"a\", \"b\", \"l\")\n        # Condition 11\n        conditions[[\"C11\"]] <- data.frame(c(2,50,180),c(1,2,3),c(0.2,0.3,0.5))\n        colnames(conditions[[\"C11\"]]) <- c(\"a\", \"b\", \"l\")\n        # Condition 12\n        conditions[[\"C12\"]] <- data.frame(c(2,50,180),c(1,2,3),c(0.2,0.3,0.5))\n        colnames(conditions[[\"C12\"]]) <- c(\"a\", \"b\", \"l\")\n        return(conditions)\n}\nconditions <- parameters()\n\n```\n\n## Generates Sample:\nWe are using the rate not beta, which rate = 1/beta\n\n```{r}\nset.seed(111)\nsample.generation <- function(sample.size, parameters){\n        samples <- apply(parameters, 1, function(i){rgamma(sample.size* i[3], shape = i[1], scale = i[2])\n        })\n        return(unlist(samples))\n}\n\nsample <- sample.generation(100, conditions[[\"C12\"]])\n\n\n```\n\n## Parameter Estimation:\nFor each set of samples, estimate the mixture-of-gammas model using 4 different stragegies.\n\n### Strategies 1\nSpecify the starting values in gammamixEM2 using the parameter values for the simulation.\n\n```{r}\nestimation1.f <- function(dat, para){\n        a <- para$a\n        b <- para$b\n        l <- para$l\n        numOfDist <- nrow(para)\n        output <- gammamixEM2(dat, lambda = l, alpha = a, beta = b, k = numOfDist)\n        return(output[2:4])\n}\n\ntest1 <- estimation1.f(sample, conditions[[\"C12\"]])\ntest1\n```\n\n### Strategies 2\nDo not specify starting values for any of the parameters in gammamixEM2. Run the algorithm 10 times and retain the output that has the best log-likelihood value; i.e. the fit that has the largest log-likelihood value.\n```{r}\nRprof(\"profile2.out\")\nestimation2.f <- function(dat, para){\n        numOfDist <- nrow(para)\n        output <- gammamixEM2(dat, k = numOfDist, epsilon = 0.1)\n        return(output[2:4])\n}\n\ntest2 <- estimation2.f(sample, conditions[[\"C12\"]])\ntest2\n\n```\n### Strategies 3\n* Transform simulated data by taking the cub root, using normalmixEM to classify each observation to a component, which will effectively partition the simulated data into k groups. \n\n* Do this classfication\n* Meeting June 22, 16: \n   * The fitdist() function from tent's code might not stable, so we decided to use the gamma.nr(), used it to estimate the parameter after classification and transform back to gamma with ^3. \n   \n   \n   \n################ Need to ask which is parameter needs to pass in, and eps is tolerance what is the value?\n```{r}\nestimation3.f <- function(dat, para){\n        datCubeRoot <- dat^(1/3)\n        numOfDist <- nrow(para)\n        classification <- normalmixEM(datCubeRoot, k = numOfDist) ## The classified data are clustered and not like real classified.\n        clf.dat <- apply((classification$posterior == T), 2, function(x) dat[x] )\n        para.est <- lapply(c(1:numOfDist), function(i){\n                test = try(nr.gamma(x=unlist(clf.dat[i]), eps = 0.01), silent = T)\n                if (class(test)==\"try-error\"){\n                        test = as.numeric(gammamix.init(unlist(clf.dat[i]), lambda = 1, k = 1)[2:3])                \n                        return(test)}\n                else{return(test$theta)}\n                }) \n        a <- unlist(data.frame(para.est)[1,])\n        b <- unlist(data.frame(para.est)[2,])\n        l <- apply((classification$posterior == T), 2, function(x) {table(x)[\"TRUE\"]/length(x)} )\n        output <- gammamixEM2(dat, lambda = l, alpha = a, beta = b, k = numOfDist)[2:4]\n        return(output)\n}\ntest3 <- estimation3.f(sample, conditions[[\"C12\"]])\ntest3\n\n```\n### Strategies 4\nRe do the strategy 3, but set $alpha$ to the true parameter values and set $fix.alpha=TRUE$ in $gammamixEM2$.\n```{r}\ndat = sample\nnumOfDist = 3\nestimation4.f <- function(dat, para){\n        datCubeRoot <- dat^(1/3)\n        numOfDist <- nrow(para)\n        classification <- normalmixEM(datCubeRoot, k = numOfDist)\n        clf.dat <- apply((classification$posterior == T), 2, function(x) dat[x] )\n        para.est <- lapply(c(1:numOfDist), function(i){\n                test = try(nr.gamma(x=unlist(clf.dat[i]), eps = 0.01), silent = T)\n                if (class(test)==\"try-error\"){\n                        test = as.numeric(gammamix.init(unlist(clf.dat[i]), lambda = 1, k = 1)[2:3])                \n                        return(test)}\n                else{return(test$theta)}\n                }) \n        a <- para$a\n        b <- unlist(data.frame(para.est)[2,])\n        l <- apply((classification$posterior == T), 2, function(x) {table(x)[\"TRUE\"]/length(x)} )\n        output <- gammamixEM2(dat, lambda = l, alpha = a, beta = b, k = numOfDist, fix.alpha = T)[2:4]\n        return(output)\n}\ntest4 <- estimation4.f(sample, conditions[[\"C12\"]])\ntest4\n\n```\n\n## Note for simulation:\n* For each of the 4 strategies, keep a list of your output. Construct each list to be of length 5000 such that each element of the list is a matrix with the estimated parameter values and the final log-likehood. Specifiically, if out is your output, then collect your output as $new.out <- rbind(out$gamma.pars, out$lambda, out$loglik)$. Note the fourth row of the output matrix will simply be the log-likehood repeated k times.\n\n* In your lists, make sure you post-process the output by ordering the columns based on their estimates of the component means. Specifically, if you have $new.out$ as defined above, then reorder the columns using <code> new.out<-new.out[,order(new.out[1,]/new.out[2,])] </code>. We do this to avoid the label switching problem in mixture estimation.\n\n* In simulations, so the covergence creterion argument in gammamixEM2 to epsilon = 1e-5.\n\n* Make sure that simulation can recover in case of an unintened error in the EM algorithm. One way to do this is to enclose everything within a while statement. Then wrap the try funtion with option silent=FALSE (should be T?) around the gammamixEM2. Test to see if the output is of class \"try-error\". If it is, prompt your loop to return to the previous iteration. If not, let the loop increment appropriately.\n\nIn total, there are 12 matrix condition, 3 different samplesize, and 4 different starting value strategies. Total condition is 144.\n\n* For each of the 144 simulation condition, calculate the MSEs from the true parameter value as well as the mean and standard deviation of your parameter estimates; i.e., the MC standard deviation of your 5000 estimates will provide an estimate of the standard errors for each parameter.\n\n* USe parallize code and run from the Unix clusters.\n\n```{r simulation}\nlibrary(parallel)\nsimulation <- function(s.size, condition, strategy){\n        # 12 conditions\n        results <- mclapply(condition, function(para){\n                # 3 sample size\n                print(para)\n                one.iteration.results <- lapply(s.size, function(s){\n                        print(s)\n                        dat <- sample.generation(sample.size = s, parameters = para)\n                        one.sample.result <- strategy(dat, para)\n                        return(one.sample.result)\n                })\n                return(one.iteration.results)\n        })        \n        return(results)\n}\n\ntest.sim <- simulation(sample.size, conditions, estimation1.f)\n```\n",
    "created" : 1467595622823.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3299004707",
    "id" : "18C51D10",
    "lastKnownWriteTime" : 1467596698,
    "path" : "C:/Users/xch234/Desktop/Mix_Gammas/Code/Mix_Gammas_V1.Rmd",
    "project_path" : "Code/Mix_Gammas_V1.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}